{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import os.path\n",
    "import os\n",
    "import fnmatch\n",
    "from numpy import cumsum\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "import itertools\n",
    "from matplotlib.pyplot import *\n",
    "from textwrap import wrap\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import random\n",
    "import re\n",
    "import csv\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('This software generates a figure of profile clusters on a selected corpus \\nusage: --dir path/to/your/source/dir --figname name_for_your_fig')\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dir', help= '/your/directory/to/tagged/files/')\n",
    "parser.add_argument('--figname', help= 'name_for_your_fig')\n",
    "args = parser.parse_args()\n",
    "if len(sys.argv) == 1:\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argsdir= os.path.join(args.dir, '')\n",
    "argsfig= os.path.join(args.figname, '')\n",
    "files_list=fnmatch.filter(os.listdir(path_to_folder), '*.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_folder = \"/home/odysseus/Bureau/ANR/corpus/tagged_zola/\"\n",
    "argsfig='clusters'\n",
    "directory_dataset='dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list=fnmatch.filter(os.listdir(path_to_folder), '*.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclid_dist(t1,t2):\n",
    "    return np.sqrt(sum((t1-t2)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DTWDistance(s1, s2):\n",
    "    DTW={}\n",
    "    \n",
    "    for i in range(len(s1)):\n",
    "        DTW[(i, -1)] = float('inf')\n",
    "    for i in range(len(s2)):\n",
    "        DTW[(-1, i)] = float('inf')\n",
    "    DTW[(-1, -1)] = 0\n",
    "\n",
    "    for i in range(len(s1)):\n",
    "        for j in range(len(s2)):\n",
    "            dist= (s1[i]-s2[j])**2\n",
    "            DTW[(i, j)] = dist + min(DTW[(i-1, j)],DTW[(i, j-1)], DTW[(i-1, j-1)])\n",
    "    return np.sqrt(DTW[len(s1)-1, len(s2)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DTWDistance(s1, s2,w):\n",
    "    DTW={}\n",
    "    \n",
    "    w = max(w, abs(len(s1)-len(s2)))\n",
    "    \n",
    "    for i in range(-1,len(s1)):\n",
    "        for j in range(-1,len(s2)):\n",
    "            DTW[(i, j)] = float('inf')\n",
    "    DTW[(-1, -1)] = 0\n",
    "  \n",
    "    for i in range(len(s1)):\n",
    "        for j in range(max(0, i-w), min(len(s2), i+w)):\n",
    "            dist= (s1[i]-s2[j])**2\n",
    "            DTW[(i, j)] = dist + min(DTW[(i-1, j)],DTW[(i, j-1)], DTW[(i-1, j-1)])\n",
    "\n",
    "    return np.sqrt(DTW[len(s1)-1, len(s2)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LB_Keogh(s1,s2,r):\n",
    "    LB_sum=0\n",
    "    for ind,i in enumerate(s1):\n",
    "        \n",
    "        lower_bound=min(s2[(ind-r if ind-r>=0 else 0):(ind+r)])\n",
    "        upper_bound=max(s2[(ind-r if ind-r>=0 else 0):(ind+r)])\n",
    "        \n",
    "        if i>upper_bound:\n",
    "            LB_sum=LB_sum+(i-upper_bound)**2\n",
    "        elif i<lower_bound:\n",
    "            LB_sum=LB_sum+(i-lower_bound)**2\n",
    "    \n",
    "    return np.sqrt(LB_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(train,test,w):\n",
    "    preds=[]\n",
    "    for ind,i in enumerate(test):\n",
    "        min_dist=float('inf')\n",
    "        closest_seq=[]\n",
    "        #print ind\n",
    "        for j in train:\n",
    "            if LB_Keogh(i[:-1],j[:-1],5)<min_dist:\n",
    "                dist=DTWDistance(i[:-1],j[:-1],w)\n",
    "                if dist<min_dist:\n",
    "                    min_dist=dist\n",
    "                    closest_seq=j\n",
    "        preds.append(closest_seq[-1])\n",
    "    return classification_report(test[:,-1],preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_clust(data,names, num_clust,num_iter,w):  \n",
    "    counter=0\n",
    "    centroids=random.sample(list(data),num_clust)\n",
    "    \n",
    "    for n in range(num_iter):\n",
    "        counter+=1\n",
    "        assignments={}\n",
    "        mapClust={}\n",
    "        #assign data points to clusters\n",
    "        for ind,i in enumerate(data):\n",
    "            min_dist=float('inf')\n",
    "            closest_clust=None\n",
    "            for c_ind,j in enumerate(centroids):\n",
    "                if LB_Keogh(i,j,5)<min_dist:\n",
    "                    cur_dist=DTWDistance(i,j,w)\n",
    "                    if cur_dist<min_dist:\n",
    "                        min_dist=cur_dist\n",
    "                        closest_clust=c_ind\n",
    "            if closest_clust in assignments:\n",
    "                assignments[closest_clust].append(ind)\n",
    "            else:\n",
    "                assignments[closest_clust]=[]\n",
    "    \n",
    "        #recalculate centroids of clusters\n",
    "        for key in assignments:\n",
    "            nameString=''\n",
    "            clust_sum=0\n",
    "            for k in assignments[key]:\n",
    "                clust_sum=clust_sum+data[k]\n",
    "                name=names[k].replace('_',' ')\n",
    "                if name not in nameString:\n",
    "                    nameString+=name+' // '\n",
    "            centroids[key]=[m/len(assignments[key]) for m in clust_sum]\n",
    "            mapClust[nameString]=centroids[key]\n",
    "        \n",
    "    return mapClust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movingaverage(interval, window_size):\n",
    "    window= np.ones(int(window_size))/float(window_size)\n",
    "    return np.convolve(interval, window, 'same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1872_Zola-Emile_La-curee.xml\n",
      "1882_Zola-Emile_Pot-Bouille.xml\n",
      "1885_Zola-Emile_Germinal.xml\n",
      "1877_Zola-Emile_L-assomoir.xml\n",
      "1875_Zola-Emile_La-faute-de-l-abbe-Mouret.xml\n",
      "1880_Zola-Emile_Nana.xml\n",
      "1878_Zola-Emile_Une-page-d-amour.xml\n",
      "1870_Zola-Emile_La-fortune-des-Rougon.xml\n",
      "1883_Zola-Emile_Au-bonheur-des-dames.xml\n"
     ]
    }
   ],
   "source": [
    "chaps=list()\n",
    "names=list()\n",
    "data_per_name=list()\n",
    "\n",
    "for file in files_list:\n",
    "    print(file)\n",
    "    global_nb_words = 0\n",
    "    tmpFile=file.replace(\"/\",\":\")\n",
    "    full_path=path_to_folder+tmpFile\n",
    "    if os.path.isfile(full_path):\n",
    "        tree=etree.parse(full_path)\n",
    "        nb_words_per_chap=list()\n",
    "        if tree.findall(\".//div[@type='chapter']\"):\n",
    "            nb_chaps = len(tree.findall(\".//div[@type='chapter']\"))\n",
    "            for idx, chap in enumerate(tree.findall(\".//div[@type='chapter']\")):\n",
    "                nb_words = len(chap.findall(\".//word\"))\n",
    "                global_nb_words+= nb_words\n",
    "                nb_words_per_chap.append(nb_words)\n",
    "        \n",
    "            list_perc_per_chap=list()\n",
    "            cum_l = np.cumsum(nb_words_per_chap)\n",
    "            for idx,num in enumerate(cum_l):\n",
    "                if idx>0:\n",
    "                    cur_num = (num/global_nb_words)*100\n",
    "                    prev_num = (cum_l[idx-1]/global_nb_words)*100\n",
    "                    list_perc_per_chap.append(cur_num - prev_num)\n",
    "                else:\n",
    "                    list_perc_per_chap.append((num/global_nb_words)*100)\n",
    "              \n",
    "            num_div = 101/len(list_perc_per_chap)\n",
    "            \n",
    "            perc_chap_100=dict()\n",
    "            val=0\n",
    "            num_div_prev=num_div\n",
    "            \n",
    "            index=0\n",
    "            for i in range(0,101):      \n",
    "                if i > num_div_prev:\n",
    "                    num_div_prev+=num_div\n",
    "                    index+=1\n",
    "                perc_chap_100[i]=(list_perc_per_chap[index]/num_div)\n",
    "            \n",
    "            names.append(re.sub(u'\\n','',tree.find(\".//title\").text).replace(\"     \",\"\").replace(\" \",\"_\"))\n",
    "            \n",
    "            data_per_name.append(perc_chap_100.values())          \n",
    "            \n",
    "        else :\n",
    "            print(\"!!! No chapter found in file : \"+file)         \n",
    "\n",
    "with open('datasets/names.txt', 'w') as f:\n",
    "    f.write((' '.join(names*4)))\n",
    "    f.close()\n",
    "writer=csv.writer(open('datasets/data.csv','w'),delimiter='\\t')\n",
    "for i in range (0,4):\n",
    "    for percs in data_per_name:\n",
    "        writer.writerow(percs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters=5\n",
    "num_iters=50\n",
    "w=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [16,9]\n",
    "test = np.genfromtxt('datasets/data.csv', delimiter='\\t')\n",
    "with open('datasets/names.txt', 'r') as namesFile:\n",
    "    stringNames=namesFile.read()\n",
    "namesArray=stringNames.split(' ')\n",
    "\n",
    "mapOfTimeSeries=k_means_clust(test,namesArray,num_clusters,num_iters,w)\n",
    "values=list(mapOfTimeSeries.values())\n",
    "keys=list(mapOfTimeSeries.keys())\n",
    "fig=plt.figure()\n",
    "for idx,i in enumerate(mapOfTimeSeries.values()):\n",
    "\n",
    "    keys = [ '\\n'.join(wrap(l, 50)) for l in keys ]\n",
    "    key = keys[idx]\n",
    "    p=plt.plot(movingaverage(i, 1),label=key)\n",
    "    legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=1,\n",
    "       ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "\n",
    "fig.savefig(argsfig+'.png')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
